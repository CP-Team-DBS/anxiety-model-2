{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1801e5d4",
   "metadata": {},
   "source": [
    "# 1. Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "94a4c419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import libraries (sama seperti sebelumnya)\n",
    "from empath import Empath\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import contractions\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model # Menggunakan Model API\n",
    "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, Dense, Dropout # Pastikan Input ada di sini\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanSquaredError # Untuk regression\n",
    "from tensorflow.keras.metrics import MeanAbsoluteError, MeanSquaredError as KerasMSE # Opsional: import metrik secara eksplisit\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import tensorflow as tf # Untuk tf.keras.models.load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c5488011",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\laila\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\laila\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\laila\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Inisialisasi\n",
    "lexicon = Empath()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "custom_stopwords = {'like', 'get', 'go', 'know', 'would', 'could', 'also'}\n",
    "stop_words.update(custom_stopwords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea89fb2",
   "metadata": {},
   "source": [
    "# 2. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9332b07a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>statement</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Final doctor appointment tomorrow, tired of co...</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Anyone have bone or muscle pain that was stres...</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Listening to your body? I'm curious how those ...</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Weekly /r/HealthAnxiety Challenge - Exercise A...</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is killing me So i had a bacterial stomac...</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           statement    label\n",
       "0  Final doctor appointment tomorrow, tired of co...  Anxiety\n",
       "1  Anyone have bone or muscle pain that was stres...  Anxiety\n",
       "2  Listening to your body? I'm curious how those ...  Anxiety\n",
       "3  Weekly /r/HealthAnxiety Challenge - Exercise A...  Anxiety\n",
       "4  This is killing me So i had a bacterial stomac...  Anxiety"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"dataset.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2d23a8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4810 entries, 0 to 4809\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   statement  4810 non-null   object\n",
      " 1   label      4810 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 75.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfb10b8",
   "metadata": {},
   "source": [
    "# 3. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e52e6a7",
   "metadata": {},
   "source": [
    "## 3.1. Cleaning n lemmetizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5882ad0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()  # Mengubah teks menjadi huruf kecil \n",
    "    text = contractions.fix(text)  # Memperbaiki kontraksi \n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)  # Menghapus URL \n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)  # Menghapus karakter non-ASCII \n",
    "    text = re.sub(r'\\d+', '', text)  # Menghapus angka \n",
    "    text = re.sub(rf\"[{re.escape(string.punctuation)}]\", '', text)  # Menghapus tanda baca \n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Ganti multiple spasi dengan 1 spasi)\n",
    "    return text  # Mengembalikan teks yang sudah dibersihkan\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = clean_text(text) \n",
    "    words = text.split()  # Memisahkan teks menjadi list kata \n",
    "    processed_words = []  # List untuk menyimpan kata yang sudah diproses\n",
    "    for word in words:\n",
    "        if word not in stop_words and len(word) > 2:  # Filter: hapus stopword dan kata dengan panjang ≤ 2\n",
    "            lemma = lemmatizer.lemmatize(word, pos='v')  # Lemmatisasi sebagai verb (e.g., \"running\" → \"run\")\n",
    "            lemma = lemmatizer.lemmatize(lemma, pos='n')  # Lemmatisasi sebagai noun (e.g., \"wolves\" → \"wolf\")\n",
    "            lemma = lemmatizer.lemmatize(lemma, pos='a')  # Lemmatisasi sebagai adjective (e.g., \"better\" → \"good\")\n",
    "            lemma = lemmatizer.lemmatize(lemma, pos='r')  # Lemmatisasi sebagai adverb (e.g., \"quickly\" → \"quick\")\n",
    "            processed_words.append(lemma)  # Tambahkan kata yang sudah dilematisasi ke list\n",
    "    return ' '.join(processed_words)  # Gabungkan list kata menjadi teks dengan spasi\n",
    "\n",
    "df['statement'] = df['statement'].astype(str)\n",
    "df['cleaned_statement'] = df['statement'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b38b0d4",
   "metadata": {},
   "source": [
    "## 3.2. Emotion Extraction with Emapth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1eb3529f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daftar emosi yang digunakan\n",
    "emotions = ['anxiety', 'fear', 'nervousness', 'sadness', 'suffering', 'shame']\n",
    "\n",
    "# Mapping keyword per emosi\n",
    "keyword_emotion_map = {\n",
    "    'anxiety': ['anxious', 'nervous', 'overwhelmed', 'restless', 'panic', 'worried'],\n",
    "    'fear': ['fear', 'scared', 'terrified', 'afraid', 'panic'],\n",
    "    'nervousness': ['nervous', 'dizzy', 'shaky', 'jittery', 'restless'],\n",
    "    'sadness': ['sad', 'cry', 'heartbroken', 'hopeless', 'misery', 'despair', 'alone'],\n",
    "    'suffering': ['suffering', 'pain', 'hurt', 'agony', 'tired'],\n",
    "    'shame': ['shame', 'guilt', 'embarrassed', 'worthless', 'regret']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1982d718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Fungsi-fungsi Empath dan Boosting ---\n",
    "def label_from_empath(text):\n",
    "    scores = lexicon.analyze(text, categories=emotions, normalize=True)\n",
    "    return scores\n",
    "\n",
    "def boost_empath_scores_with_keywords(text, empath_scores, keyword_emotion_map):\n",
    "    text_lower = text.lower()\n",
    "    boosted_scores = empath_scores.copy()\n",
    "\n",
    "    for emotion, keywords in keyword_emotion_map.items():\n",
    "        for kw in keywords:\n",
    "            if re.search(rf'\\b{kw}\\b', text_lower):\n",
    "                boosted_scores[emotion] = boosted_scores.get(emotion, 0) + 0.15\n",
    "\n",
    "    return boosted_scores\n",
    "\n",
    "# Fungsi konversi skor ke level (HANYA untuk pasca-pemrosesan dan visualisasi)\n",
    "def score_to_level(score):\n",
    "    if score == 0:\n",
    "        return 0\n",
    "    elif score <= 0.05:\n",
    "        return 1\n",
    "    elif score <= 0.25:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "# Ambil 3 emosi teratas, ubah ke level, sisanya level 0 (HANYA untuk pasca-pemrosesan dan visualisasi)\n",
    "def top_3_level_emotions(empath_scores_dict):\n",
    "    if not isinstance(empath_scores_dict, dict):\n",
    "        return {emotion: 0 for emotion in emotions}\n",
    "\n",
    "    top3_items = sorted([item for item in empath_scores_dict.items() if item[0] in emotions],\n",
    "                        key=lambda x: x[1],\n",
    "                        reverse=True)[:3]\n",
    "\n",
    "    result = {emotion: 0 for emotion in emotions}\n",
    "    for emo, score in top3_items:\n",
    "        result[emo] = score_to_level(score)\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aca235c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Penerapan Empath dan Boosting pada DataFrame ---\n",
    "# 1. Skor awal dari Empath\n",
    "df['empath_scores'] = df['cleaned_statement'].apply(label_from_empath)\n",
    "\n",
    "# 2. Boost dengan keyword\n",
    "df['boosted_scores'] = df.apply(\n",
    "    lambda row: boost_empath_scores_with_keywords(\n",
    "        row['cleaned_statement'], row['empath_scores'], keyword_emotion_map\n",
    "    ), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3fec18b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>statement</th>\n",
       "      <th>label</th>\n",
       "      <th>cleaned_statement</th>\n",
       "      <th>empath_scores</th>\n",
       "      <th>boosted_scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Final doctor appointment tomorrow, tired of co...</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>final doctor appointment tomorrow tire constan...</td>\n",
       "      <td>{'anxiety': 0.0, 'fear': 0.02097902097902098, ...</td>\n",
       "      <td>{'anxiety': 0.15, 'fear': 0.17097902097902098,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Anyone have bone or muscle pain that was stres...</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>anyone bone muscle pain stressanxiety induce j...</td>\n",
       "      <td>{'anxiety': 0.0, 'fear': 0.14285714285714285, ...</td>\n",
       "      <td>{'anxiety': 0.0, 'fear': 0.14285714285714285, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Listening to your body? I'm curious how those ...</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>listen body curious health anxiety listen body...</td>\n",
       "      <td>{'anxiety': 0.0, 'fear': 0.047619047619047616,...</td>\n",
       "      <td>{'anxiety': 0.0, 'fear': 0.047619047619047616,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Weekly /r/HealthAnxiety Challenge - Exercise A...</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>weekly rhealthanxiety challenge exercise littl...</td>\n",
       "      <td>{'anxiety': 0.0, 'fear': 0.07317073170731707, ...</td>\n",
       "      <td>{'anxiety': 0.0, 'fear': 0.22317073170731705, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is killing me So i had a bacterial stomac...</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>kill bacterial stomach infectionpain right sid...</td>\n",
       "      <td>{'anxiety': 0.0, 'fear': 0.029850746268656716,...</td>\n",
       "      <td>{'anxiety': 0.0, 'fear': 0.029850746268656716,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4805</th>\n",
       "      <td>Nobody takes me seriously I’ve (24M) dealt wit...</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>nobody take seriously deal depressionanxiety y...</td>\n",
       "      <td>{'anxiety': 0.0, 'fear': 0.0, 'nervousness': 0...</td>\n",
       "      <td>{'anxiety': 0.0, 'fear': 0.0, 'nervousness': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4806</th>\n",
       "      <td>selfishness  \"I don't feel very good, it's lik...</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>selfishness feel good belong world think ever ...</td>\n",
       "      <td>{'anxiety': 0.0, 'fear': 0.0, 'nervousness': 0...</td>\n",
       "      <td>{'anxiety': 0.0, 'fear': 0.0, 'nervousness': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4807</th>\n",
       "      <td>Is there any way to sleep better? I can't slee...</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>way sleep good cannot sleep night med help</td>\n",
       "      <td>{'anxiety': 0.0, 'fear': 0.0, 'nervousness': 0...</td>\n",
       "      <td>{'anxiety': 0.0, 'fear': 0.0, 'nervousness': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4808</th>\n",
       "      <td>Public speaking tips? Hi, all. I have to give ...</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>public speak tip give presentation work next w...</td>\n",
       "      <td>{'anxiety': 0.0, 'fear': 0.08823529411764706, ...</td>\n",
       "      <td>{'anxiety': 0.15, 'fear': 0.23823529411764705,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4809</th>\n",
       "      <td>I have really bad door anxiety! It's not about...</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>really bad door anxiety scar lock door somethi...</td>\n",
       "      <td>{'anxiety': 0.0, 'fear': 0.02631578947368421, ...</td>\n",
       "      <td>{'anxiety': 0.0, 'fear': 0.02631578947368421, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4810 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              statement    label  \\\n",
       "0     Final doctor appointment tomorrow, tired of co...  Anxiety   \n",
       "1     Anyone have bone or muscle pain that was stres...  Anxiety   \n",
       "2     Listening to your body? I'm curious how those ...  Anxiety   \n",
       "3     Weekly /r/HealthAnxiety Challenge - Exercise A...  Anxiety   \n",
       "4     This is killing me So i had a bacterial stomac...  Anxiety   \n",
       "...                                                 ...      ...   \n",
       "4805  Nobody takes me seriously I’ve (24M) dealt wit...  Anxiety   \n",
       "4806  selfishness  \"I don't feel very good, it's lik...  Anxiety   \n",
       "4807  Is there any way to sleep better? I can't slee...  Anxiety   \n",
       "4808  Public speaking tips? Hi, all. I have to give ...  Anxiety   \n",
       "4809  I have really bad door anxiety! It's not about...  Anxiety   \n",
       "\n",
       "                                      cleaned_statement  \\\n",
       "0     final doctor appointment tomorrow tire constan...   \n",
       "1     anyone bone muscle pain stressanxiety induce j...   \n",
       "2     listen body curious health anxiety listen body...   \n",
       "3     weekly rhealthanxiety challenge exercise littl...   \n",
       "4     kill bacterial stomach infectionpain right sid...   \n",
       "...                                                 ...   \n",
       "4805  nobody take seriously deal depressionanxiety y...   \n",
       "4806  selfishness feel good belong world think ever ...   \n",
       "4807         way sleep good cannot sleep night med help   \n",
       "4808  public speak tip give presentation work next w...   \n",
       "4809  really bad door anxiety scar lock door somethi...   \n",
       "\n",
       "                                          empath_scores  \\\n",
       "0     {'anxiety': 0.0, 'fear': 0.02097902097902098, ...   \n",
       "1     {'anxiety': 0.0, 'fear': 0.14285714285714285, ...   \n",
       "2     {'anxiety': 0.0, 'fear': 0.047619047619047616,...   \n",
       "3     {'anxiety': 0.0, 'fear': 0.07317073170731707, ...   \n",
       "4     {'anxiety': 0.0, 'fear': 0.029850746268656716,...   \n",
       "...                                                 ...   \n",
       "4805  {'anxiety': 0.0, 'fear': 0.0, 'nervousness': 0...   \n",
       "4806  {'anxiety': 0.0, 'fear': 0.0, 'nervousness': 0...   \n",
       "4807  {'anxiety': 0.0, 'fear': 0.0, 'nervousness': 0...   \n",
       "4808  {'anxiety': 0.0, 'fear': 0.08823529411764706, ...   \n",
       "4809  {'anxiety': 0.0, 'fear': 0.02631578947368421, ...   \n",
       "\n",
       "                                         boosted_scores  \n",
       "0     {'anxiety': 0.15, 'fear': 0.17097902097902098,...  \n",
       "1     {'anxiety': 0.0, 'fear': 0.14285714285714285, ...  \n",
       "2     {'anxiety': 0.0, 'fear': 0.047619047619047616,...  \n",
       "3     {'anxiety': 0.0, 'fear': 0.22317073170731705, ...  \n",
       "4     {'anxiety': 0.0, 'fear': 0.029850746268656716,...  \n",
       "...                                                 ...  \n",
       "4805  {'anxiety': 0.0, 'fear': 0.0, 'nervousness': 0...  \n",
       "4806  {'anxiety': 0.0, 'fear': 0.0, 'nervousness': 0...  \n",
       "4807  {'anxiety': 0.0, 'fear': 0.0, 'nervousness': 0...  \n",
       "4808  {'anxiety': 0.15, 'fear': 0.23823529411764705,...  \n",
       "4809  {'anxiety': 0.0, 'fear': 0.02631578947368421, ...  \n",
       "\n",
       "[4810 rows x 5 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6dc339",
   "metadata": {},
   "source": [
    "# 4. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d458dc8",
   "metadata": {},
   "source": [
    "## 4.1. Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bc62c023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melakukan tokenisasi...\n",
      "Melakukan padding...\n",
      "Ukuran training set: 2886 sampel\n",
      "Ukuran validation set: 962 sampel\n",
      "Ukuran test set: 962 sampel\n"
     ]
    }
   ],
   "source": [
    "# 4. Modeling\n",
    "# 4.1. Preprocessing data untuk Model\n",
    "X = df['cleaned_statement'].values\n",
    "\n",
    "# --- PERUBAHAN PENTING DI SINI ---\n",
    "# Target y adalah skor kontinu dari 'boosted_scores'\n",
    "y_df_continuous = pd.DataFrame(df['boosted_scores'].tolist())\n",
    "y = y_df_continuous[emotions].values # Ini adalah target kontinu Anda!\n",
    "\n",
    "# Parameter Tokenizer dan Padding\n",
    "vocab_size = 10000\n",
    "oov_token = '<OOV>'\n",
    "maxlen = 100\n",
    "\n",
    "# Tokenisasi\n",
    "print(\"Melakukan tokenisasi...\")\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_token)\n",
    "tokenizer.fit_on_texts(X)\n",
    "X_seq = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "# Padding\n",
    "print(\"Melakukan padding...\")\n",
    "X_pad = pad_sequences(X_seq, maxlen=maxlen, padding='post', truncating='post')\n",
    "\n",
    "# Pembagian Data (Train, Validation, Test)\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X_pad, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.25, random_state=42)\n",
    "\n",
    "print(f\"Ukuran training set: {X_train.shape[0]} sampel\")\n",
    "print(f\"Ukuran validation set: {X_val.shape[0]} sampel\")\n",
    "print(f\"Ukuran test set: {X_test.shape[0]} sampel\")\n",
    "\n",
    "num_emotions = y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61e319b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Model Summary ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">263,168</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">164,352</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">390</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m1,280,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_2 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │       \u001b[38;5;34m263,168\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_3 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m164,352\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │           \u001b[38;5;34m390\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,716,166</span> (6.55 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,716,166\u001b[0m (6.55 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,716,166</span> (6.55 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,716,166\u001b[0m (6.55 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Definisi Model (Tidak Berubah dari sebelumnya, sudah cocok untuk regresi) ---\n",
    "embedding_dim = 128\n",
    "\n",
    "input_layer = Input(shape=(maxlen,)) # Input layer tetap didefinisikan dengan maxlen\n",
    "embedding_layer = Embedding(input_dim=vocab_size,\n",
    "                            output_dim=embedding_dim)(input_layer)  \n",
    "\n",
    "bilstm_1 = Bidirectional(LSTM(128, return_sequences=True))(embedding_layer)\n",
    "dropout_1 = Dropout(0.5)(bilstm_1)\n",
    "\n",
    "bilstm_2 = Bidirectional(LSTM(64))(dropout_1)\n",
    "dropout_2 = Dropout(0.5)(bilstm_2)\n",
    "\n",
    "dense_hidden = Dense(64, activation='relu')(dropout_2)\n",
    "\n",
    "# Output Layer: activation='linear' dan loss=MeanSquaredError() adalah BENAR untuk regresi\n",
    "output_layer = Dense(num_emotions, activation='linear')(dense_hidden)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Compile Model\n",
    "model.compile(\n",
    "    loss=MeanSquaredError(),\n",
    "    optimizer=Adam(learning_rate=1e-3),\n",
    "    metrics=[MeanSquaredError(), 'mae']\n",
    ")\n",
    "\n",
    "print(\"\\n--- Model Summary ---\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2b873b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Callbacks untuk Pelatihan Optimal ---\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    'basic_emotion_regression_model.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b38d61fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Mulai Pelatihan Model ---\n",
      "Epoch 1/50\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 266ms/step - loss: 0.0085 - mae: 0.0658 - mean_squared_error: 0.0085\n",
      "Epoch 1: val_loss improved from inf to 0.00565, saving model to basic_emotion_regression_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 335ms/step - loss: 0.0085 - mae: 0.0658 - mean_squared_error: 0.0085 - val_loss: 0.0057 - val_mae: 0.0546 - val_mean_squared_error: 0.0064\n",
      "Epoch 2/50\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 289ms/step - loss: 0.0051 - mae: 0.0513 - mean_squared_error: 0.0051\n",
      "Epoch 2: val_loss improved from 0.00565 to 0.00438, saving model to basic_emotion_regression_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 324ms/step - loss: 0.0051 - mae: 0.0513 - mean_squared_error: 0.0051 - val_loss: 0.0044 - val_mae: 0.0455 - val_mean_squared_error: 0.0049\n",
      "Epoch 3/50\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 275ms/step - loss: 0.0034 - mae: 0.0420 - mean_squared_error: 0.0034\n",
      "Epoch 3: val_loss improved from 0.00438 to 0.00428, saving model to basic_emotion_regression_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 319ms/step - loss: 0.0034 - mae: 0.0420 - mean_squared_error: 0.0034 - val_loss: 0.0043 - val_mae: 0.0438 - val_mean_squared_error: 0.0048\n",
      "Epoch 4/50\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 268ms/step - loss: 0.0028 - mae: 0.0379 - mean_squared_error: 0.0028\n",
      "Epoch 4: val_loss improved from 0.00428 to 0.00403, saving model to basic_emotion_regression_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 306ms/step - loss: 0.0028 - mae: 0.0378 - mean_squared_error: 0.0028 - val_loss: 0.0040 - val_mae: 0.0429 - val_mean_squared_error: 0.0047\n",
      "Epoch 5/50\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 277ms/step - loss: 0.0023 - mae: 0.0349 - mean_squared_error: 0.0023\n",
      "Epoch 5: val_loss improved from 0.00403 to 0.00394, saving model to basic_emotion_regression_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 315ms/step - loss: 0.0023 - mae: 0.0349 - mean_squared_error: 0.0023 - val_loss: 0.0039 - val_mae: 0.0436 - val_mean_squared_error: 0.0044\n",
      "Epoch 6/50\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 307ms/step - loss: 0.0019 - mae: 0.0314 - mean_squared_error: 0.0019\n",
      "Epoch 6: val_loss improved from 0.00394 to 0.00349, saving model to basic_emotion_regression_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 349ms/step - loss: 0.0019 - mae: 0.0314 - mean_squared_error: 0.0019 - val_loss: 0.0035 - val_mae: 0.0393 - val_mean_squared_error: 0.0041\n",
      "Epoch 7/50\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 296ms/step - loss: 0.0016 - mae: 0.0289 - mean_squared_error: 0.0016\n",
      "Epoch 7: val_loss did not improve from 0.00349\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 328ms/step - loss: 0.0016 - mae: 0.0289 - mean_squared_error: 0.0016 - val_loss: 0.0035 - val_mae: 0.0395 - val_mean_squared_error: 0.0041\n",
      "Epoch 8/50\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 277ms/step - loss: 0.0014 - mae: 0.0271 - mean_squared_error: 0.0014\n",
      "Epoch 8: val_loss improved from 0.00349 to 0.00337, saving model to basic_emotion_regression_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 313ms/step - loss: 0.0014 - mae: 0.0271 - mean_squared_error: 0.0014 - val_loss: 0.0034 - val_mae: 0.0375 - val_mean_squared_error: 0.0040\n",
      "Epoch 9/50\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 264ms/step - loss: 0.0015 - mae: 0.0273 - mean_squared_error: 0.0015\n",
      "Epoch 9: val_loss improved from 0.00337 to 0.00332, saving model to basic_emotion_regression_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 299ms/step - loss: 0.0015 - mae: 0.0273 - mean_squared_error: 0.0015 - val_loss: 0.0033 - val_mae: 0.0373 - val_mean_squared_error: 0.0041\n",
      "Epoch 10/50\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 269ms/step - loss: 0.0013 - mae: 0.0255 - mean_squared_error: 0.0013\n",
      "Epoch 10: val_loss did not improve from 0.00332\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 301ms/step - loss: 0.0013 - mae: 0.0255 - mean_squared_error: 0.0013 - val_loss: 0.0034 - val_mae: 0.0368 - val_mean_squared_error: 0.0041\n",
      "Epoch 11/50\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 260ms/step - loss: 0.0012 - mae: 0.0244 - mean_squared_error: 0.0012\n",
      "Epoch 11: val_loss improved from 0.00332 to 0.00325, saving model to basic_emotion_regression_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 291ms/step - loss: 0.0012 - mae: 0.0244 - mean_squared_error: 0.0012 - val_loss: 0.0033 - val_mae: 0.0362 - val_mean_squared_error: 0.0039\n",
      "Epoch 12/50\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 277ms/step - loss: 0.0011 - mae: 0.0235 - mean_squared_error: 0.0011\n",
      "Epoch 12: val_loss did not improve from 0.00325\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 308ms/step - loss: 0.0011 - mae: 0.0235 - mean_squared_error: 0.0011 - val_loss: 0.0034 - val_mae: 0.0362 - val_mean_squared_error: 0.0042\n",
      "Epoch 13/50\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261ms/step - loss: 0.0011 - mae: 0.0236 - mean_squared_error: 0.0011\n",
      "Epoch 13: val_loss did not improve from 0.00325\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 300ms/step - loss: 0.0011 - mae: 0.0236 - mean_squared_error: 0.0011 - val_loss: 0.0034 - val_mae: 0.0366 - val_mean_squared_error: 0.0041\n",
      "Epoch 14/50\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 326ms/step - loss: 0.0011 - mae: 0.0241 - mean_squared_error: 0.0011\n",
      "Epoch 14: val_loss did not improve from 0.00325\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 368ms/step - loss: 0.0011 - mae: 0.0241 - mean_squared_error: 0.0011 - val_loss: 0.0033 - val_mae: 0.0363 - val_mean_squared_error: 0.0040\n",
      "Epoch 15/50\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 310ms/step - loss: 0.0010 - mae: 0.0223 - mean_squared_error: 0.0010\n",
      "Epoch 15: val_loss did not improve from 0.00325\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 347ms/step - loss: 0.0010 - mae: 0.0223 - mean_squared_error: 0.0010 - val_loss: 0.0033 - val_mae: 0.0354 - val_mean_squared_error: 0.0040\n",
      "Epoch 16/50\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 282ms/step - loss: 0.0010 - mae: 0.0223 - mean_squared_error: 0.0010\n",
      "Epoch 16: val_loss did not improve from 0.00325\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 319ms/step - loss: 0.0010 - mae: 0.0223 - mean_squared_error: 0.0010 - val_loss: 0.0033 - val_mae: 0.0354 - val_mean_squared_error: 0.0041\n",
      "Epoch 17/50\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 281ms/step - loss: 9.3917e-04 - mae: 0.0217 - mean_squared_error: 9.3937e-04\n",
      "Epoch 17: val_loss did not improve from 0.00325\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 318ms/step - loss: 9.3955e-04 - mae: 0.0217 - mean_squared_error: 9.3993e-04 - val_loss: 0.0033 - val_mae: 0.0381 - val_mean_squared_error: 0.0040\n",
      "Epoch 18/50\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 280ms/step - loss: 9.9103e-04 - mae: 0.0225 - mean_squared_error: 9.9100e-04\n",
      "Epoch 18: val_loss improved from 0.00325 to 0.00323, saving model to basic_emotion_regression_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 320ms/step - loss: 9.8957e-04 - mae: 0.0225 - mean_squared_error: 9.8952e-04 - val_loss: 0.0032 - val_mae: 0.0348 - val_mean_squared_error: 0.0039\n",
      "Epoch 19/50\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 291ms/step - loss: 8.4432e-04 - mae: 0.0204 - mean_squared_error: 8.4488e-04\n",
      "Epoch 19: val_loss did not improve from 0.00323\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 328ms/step - loss: 8.4478e-04 - mae: 0.0204 - mean_squared_error: 8.4588e-04 - val_loss: 0.0033 - val_mae: 0.0356 - val_mean_squared_error: 0.0040\n",
      "Epoch 20/50\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 288ms/step - loss: 9.3857e-04 - mae: 0.0214 - mean_squared_error: 9.3837e-04\n",
      "Epoch 20: val_loss did not improve from 0.00323\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 320ms/step - loss: 9.3874e-04 - mae: 0.0214 - mean_squared_error: 9.3834e-04 - val_loss: 0.0033 - val_mae: 0.0359 - val_mean_squared_error: 0.0040\n",
      "Epoch 21/50\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 253ms/step - loss: 8.7426e-04 - mae: 0.0208 - mean_squared_error: 8.7439e-04\n",
      "Epoch 21: val_loss did not improve from 0.00323\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 284ms/step - loss: 8.7390e-04 - mae: 0.0208 - mean_squared_error: 8.7415e-04 - val_loss: 0.0033 - val_mae: 0.0356 - val_mean_squared_error: 0.0039\n",
      "Epoch 22/50\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 254ms/step - loss: 8.3324e-04 - mae: 0.0204 - mean_squared_error: 8.3324e-04\n",
      "Epoch 22: val_loss did not improve from 0.00323\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 285ms/step - loss: 8.3293e-04 - mae: 0.0204 - mean_squared_error: 8.3293e-04 - val_loss: 0.0033 - val_mae: 0.0353 - val_mean_squared_error: 0.0040\n",
      "Epoch 23/50\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 258ms/step - loss: 8.7200e-04 - mae: 0.0207 - mean_squared_error: 8.7235e-04\n",
      "Epoch 23: val_loss did not improve from 0.00323\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 290ms/step - loss: 8.7087e-04 - mae: 0.0207 - mean_squared_error: 8.7155e-04 - val_loss: 0.0033 - val_mae: 0.0351 - val_mean_squared_error: 0.0041\n",
      "Epoch 24/50\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 264ms/step - loss: 8.8624e-04 - mae: 0.0210 - mean_squared_error: 8.8621e-04\n",
      "Epoch 24: val_loss did not improve from 0.00323\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 295ms/step - loss: 8.8588e-04 - mae: 0.0210 - mean_squared_error: 8.8581e-04 - val_loss: 0.0033 - val_mae: 0.0349 - val_mean_squared_error: 0.0040\n",
      "Epoch 25/50\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 253ms/step - loss: 7.9139e-04 - mae: 0.0198 - mean_squared_error: 7.9151e-04\n",
      "Epoch 25: val_loss improved from 0.00323 to 0.00322, saving model to basic_emotion_regression_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 287ms/step - loss: 7.9119e-04 - mae: 0.0198 - mean_squared_error: 7.9144e-04 - val_loss: 0.0032 - val_mae: 0.0343 - val_mean_squared_error: 0.0039\n",
      "Epoch 26/50\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 259ms/step - loss: 7.0474e-04 - mae: 0.0190 - mean_squared_error: 7.0497e-04\n",
      "Epoch 26: val_loss did not improve from 0.00322\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 290ms/step - loss: 7.0533e-04 - mae: 0.0190 - mean_squared_error: 7.0578e-04 - val_loss: 0.0033 - val_mae: 0.0341 - val_mean_squared_error: 0.0039\n",
      "Epoch 27/50\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 255ms/step - loss: 7.4873e-04 - mae: 0.0191 - mean_squared_error: 7.4871e-04\n",
      "Epoch 27: val_loss improved from 0.00322 to 0.00319, saving model to basic_emotion_regression_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 289ms/step - loss: 7.4805e-04 - mae: 0.0191 - mean_squared_error: 7.4800e-04 - val_loss: 0.0032 - val_mae: 0.0341 - val_mean_squared_error: 0.0038\n",
      "Epoch 28/50\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 254ms/step - loss: 6.8741e-04 - mae: 0.0183 - mean_squared_error: 6.8723e-04\n",
      "Epoch 28: val_loss did not improve from 0.00319\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 285ms/step - loss: 6.8757e-04 - mae: 0.0183 - mean_squared_error: 6.8722e-04 - val_loss: 0.0032 - val_mae: 0.0344 - val_mean_squared_error: 0.0039\n",
      "Epoch 29/50\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 252ms/step - loss: 6.8688e-04 - mae: 0.0182 - mean_squared_error: 6.8687e-04\n",
      "Epoch 29: val_loss did not improve from 0.00319\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 283ms/step - loss: 6.8769e-04 - mae: 0.0182 - mean_squared_error: 6.8767e-04 - val_loss: 0.0033 - val_mae: 0.0343 - val_mean_squared_error: 0.0040\n",
      "Epoch 30/50\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 253ms/step - loss: 6.6443e-04 - mae: 0.0182 - mean_squared_error: 6.6447e-04\n",
      "Epoch 30: val_loss did not improve from 0.00319\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 284ms/step - loss: 6.6482e-04 - mae: 0.0182 - mean_squared_error: 6.6491e-04 - val_loss: 0.0032 - val_mae: 0.0343 - val_mean_squared_error: 0.0040\n",
      "Epoch 31/50\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 256ms/step - loss: 7.2824e-04 - mae: 0.0187 - mean_squared_error: 7.2804e-04\n",
      "Epoch 31: val_loss did not improve from 0.00319\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 288ms/step - loss: 7.2831e-04 - mae: 0.0187 - mean_squared_error: 7.2790e-04 - val_loss: 0.0032 - val_mae: 0.0342 - val_mean_squared_error: 0.0039\n",
      "Epoch 32/50\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 253ms/step - loss: 6.5668e-04 - mae: 0.0179 - mean_squared_error: 6.5664e-04\n",
      "Epoch 32: val_loss did not improve from 0.00319\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 284ms/step - loss: 6.5684e-04 - mae: 0.0179 - mean_squared_error: 6.5677e-04 - val_loss: 0.0034 - val_mae: 0.0351 - val_mean_squared_error: 0.0040\n",
      "Epoch 33/50\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 253ms/step - loss: 7.3656e-04 - mae: 0.0187 - mean_squared_error: 7.3659e-04\n",
      "Epoch 33: val_loss did not improve from 0.00319\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 285ms/step - loss: 7.3571e-04 - mae: 0.0187 - mean_squared_error: 7.3576e-04 - val_loss: 0.0033 - val_mae: 0.0338 - val_mean_squared_error: 0.0039\n",
      "Epoch 34/50\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 252ms/step - loss: 6.8681e-04 - mae: 0.0181 - mean_squared_error: 6.8735e-04\n",
      "Epoch 34: val_loss did not improve from 0.00319\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 284ms/step - loss: 6.8712e-04 - mae: 0.0181 - mean_squared_error: 6.8818e-04 - val_loss: 0.0035 - val_mae: 0.0354 - val_mean_squared_error: 0.0042\n",
      "Epoch 35/50\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 255ms/step - loss: 7.4220e-04 - mae: 0.0188 - mean_squared_error: 7.4210e-04\n",
      "Epoch 35: val_loss did not improve from 0.00319\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 289ms/step - loss: 7.4158e-04 - mae: 0.0188 - mean_squared_error: 7.4138e-04 - val_loss: 0.0033 - val_mae: 0.0337 - val_mean_squared_error: 0.0040\n",
      "Epoch 36/50\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 251ms/step - loss: 6.4444e-04 - mae: 0.0177 - mean_squared_error: 6.4454e-04\n",
      "Epoch 36: val_loss did not improve from 0.00319\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 282ms/step - loss: 6.4455e-04 - mae: 0.0177 - mean_squared_error: 6.4475e-04 - val_loss: 0.0032 - val_mae: 0.0342 - val_mean_squared_error: 0.0041\n",
      "Epoch 37/50\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 255ms/step - loss: 6.3189e-04 - mae: 0.0175 - mean_squared_error: 6.3203e-04\n",
      "Epoch 37: val_loss did not improve from 0.00319\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 287ms/step - loss: 6.3203e-04 - mae: 0.0175 - mean_squared_error: 6.3230e-04 - val_loss: 0.0032 - val_mae: 0.0335 - val_mean_squared_error: 0.0040\n",
      "Epoch 37: early stopping\n",
      "Restoring model weights from the end of the best epoch: 27.\n"
     ]
    }
   ],
   "source": [
    "# --- Pelatihan Model ---\n",
    "print(\"\\n--- Mulai Pelatihan Model ---\")\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[early_stopping, model_checkpoint]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a2fd268d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluasi Model pada Test Set ---\n",
      "Test MSE: 0.0033\n",
      "Test MAE: 0.0342\n"
     ]
    }
   ],
   "source": [
    "# --- Evaluasi Model ---\n",
    "print(\"\\n--- Evaluasi Model pada Test Set ---\")\n",
    "try:\n",
    "    # Memuat model terbaik\n",
    "    best_model = tf.keras.models.load_model('basic_emotion_regression_model.h5', compile=False)\n",
    "    best_model.compile(loss=MeanSquaredError(), optimizer=Adam(learning_rate=1e-3), metrics=[KerasMSE(), MeanAbsoluteError()])\n",
    "except Exception as e:\n",
    "    print(f\"Gagal memuat model terbaik, menggunakan model yang terakhir dilatih: {e}\")\n",
    "    best_model = model\n",
    "\n",
    "loss, mse, mae = best_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test MSE: {mse:.4f}\")\n",
    "print(f\"Test MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5aee6961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Contoh Prediksi pada Test Set ---\n",
      "\n",
      "--- Sampel 1 ---\n",
      "Teks Asli: 'dream one move new unit anjrit worry'\n",
      "Emosi Sebenarnya (Level 0-3): {'anxiety': 0, 'fear': 0, 'nervousness': 2, 'sadness': 0, 'suffering': 0, 'shame': 2}\n",
      "Prediksi Emosi (Level 0-3): {'anxiety': 2, 'fear': 0, 'nervousness': 2, 'sadness': 0, 'suffering': 0, 'shame': 2}\n",
      "\n",
      "--- Sampel 2 ---\n",
      "Teks Asli: 'symptom without everyone think possible people symptom long time headache fatigue muscle ache without'\n",
      "Emosi Sebenarnya (Level 0-3): {'anxiety': 0, 'fear': 0, 'nervousness': 0, 'sadness': 0, 'suffering': 2, 'shame': 0}\n",
      "Prediksi Emosi (Level 0-3): {'anxiety': 0, 'fear': 0, 'nervousness': 2, 'sadness': 0, 'suffering': 2, 'shame': 2}\n",
      "\n",
      "--- Sampel 3 ---\n",
      "Teks Asli: 'brain tumor concern feel super anxious lately think worth vent moment year old feel anxious idea brain tumor headache presentish minor call bad headache life feel see tiny streak light huge glare anything mess vision feel see tiny streak sometimes sometimes feel double vision read might problem since need glass leave eyelid small twitch though right eyelid start bite wake early normal sure speech always stutter might overthinking ear ring vomit suffer seizure yet chalk allergy since hit hard season need comfort reason panic anxiety really mess even go doctor two week bite concern health'\n",
      "Emosi Sebenarnya (Level 0-3): {'anxiety': 3, 'fear': 2, 'nervousness': 2, 'sadness': 0, 'suffering': 0, 'shame': 0}\n",
      "Prediksi Emosi (Level 0-3): {'anxiety': 3, 'fear': 2, 'nervousness': 2, 'sadness': 0, 'suffering': 0, 'shame': 0}\n",
      "\n",
      "--- Sampel 4 ---\n",
      "Teks Asli: 'day day hope grow close feel expect sometimes anxious point become worry worry disturb hop especially disturb'\n",
      "Emosi Sebenarnya (Level 0-3): {'anxiety': 2, 'fear': 0, 'nervousness': 2, 'sadness': 0, 'suffering': 0, 'shame': 2}\n",
      "Prediksi Emosi (Level 0-3): {'anxiety': 2, 'fear': 0, 'nervousness': 2, 'sadness': 0, 'suffering': 0, 'shame': 2}\n",
      "\n",
      "--- Sampel 5 ---\n",
      "Teks Asli: 'might lose mind suffer health anxiety around month last month convince pelvic inflammatory disease cervical cancer arthritis periodontitis recently lymphoma swell lymph node neck feel two small lymph nod stick together worry mat typically point cancer notice lump two week ago maybe size move press every day bounce back forth anxiety calmness although say time think new disease seem real time idea'\n",
      "Emosi Sebenarnya (Level 0-3): {'anxiety': 0, 'fear': 0, 'nervousness': 2, 'sadness': 1, 'suffering': 0, 'shame': 1}\n",
      "Prediksi Emosi (Level 0-3): {'anxiety': 0, 'fear': 0, 'nervousness': 2, 'sadness': 1, 'suffering': 0, 'shame': 1}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Contoh Prediksi pada Test Set (dengan pasca-pemrosesan ke level) ---\n",
    "print(\"\\n--- Contoh Prediksi pada Test Set ---\")\n",
    "emotion_labels = ['anxiety', 'fear', 'nervousness', 'sadness', 'suffering', 'shame']\n",
    "\n",
    "num_samples_to_show = min(5, X_test.shape[0])\n",
    "for i in range(num_samples_to_show):\n",
    "    sample_text_input = X_test[i:i+1]\n",
    "    true_raw_scores = y_test[i] # Ground truth adalah skor kontinu\n",
    "\n",
    "    # --- Pasca-pemrosesan untuk mendapatkan 3 emosi dominan dalam level ---\n",
    "    predicted_raw_scores = best_model.predict(sample_text_input, verbose=0)[0]\n",
    "\n",
    "    # Dapatkan 3 emosi dominan dan levelnya\n",
    "    final_emotion_levels = top_3_level_emotions(\n",
    "        {label: score for label, score in zip(emotion_labels, predicted_raw_scores)}\n",
    "    )\n",
    "    # Untuk true emotions, kita juga perlu mengonversi dari raw score ke level 0-3 dengan logika top 3\n",
    "    true_emotion_levels = top_3_level_emotions(\n",
    "        {label: score for label, score in zip(emotion_labels, true_raw_scores)}\n",
    "    )\n",
    "\n",
    "    # Dapatkan teks asli dari dataframe (ini butuh mapping indeks)\n",
    "    original_text_idx = np.where((X_pad == sample_text_input).all(axis=1))[0]\n",
    "    original_text = df['cleaned_statement'].iloc[original_text_idx[0]] if len(original_text_idx) > 0 else \"N/A\"\n",
    "\n",
    "    print(f\"\\n--- Sampel {i+1} ---\")\n",
    "    print(f\"Teks Asli: '{original_text}'\")\n",
    "    print(f\"Emosi Sebenarnya (Level 0-3): {true_emotion_levels}\") # Kini true_emotion_levels\n",
    "    print(f\"Prediksi Emosi (Level 0-3): {final_emotion_levels}\")\n",
    "    # print(f\"Raw Prediksi (Float): {predicted_raw_scores}\") # Opsional: lihat skor mentah\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c347deef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Contoh Prediksi untuk Teks Baru ---\n",
      "Teks: 'I am truly happy today, life is great!'\n",
      "Prediksi: {'anxiety': 2, 'fear': 0, 'nervousness': 2, 'sadness': 1, 'suffering': 0, 'shame': 0}\n",
      "\n",
      "Teks: 'This situation makes me feel so helpless and trapped.'\n",
      "Prediksi: {'anxiety': 0, 'fear': 0, 'nervousness': 2, 'sadness': 0, 'suffering': 2, 'shame': 2}\n",
      "\n",
      "Teks: 'I have an exam soon, feeling a mix of nerves and excitement.'\n",
      "Prediksi: {'anxiety': 0, 'fear': 0, 'nervousness': 2, 'sadness': 1, 'suffering': 0, 'shame': 2}\n",
      "\n",
      "Teks: 'The news today filled me with deep sorrow.'\n",
      "Prediksi: {'anxiety': 2, 'fear': 0, 'nervousness': 2, 'sadness': 0, 'suffering': 0, 'shame': 1}\n",
      "\n",
      "Teks: 'I endured a lot, but I am recovering.'\n",
      "Prediksi: {'anxiety': 2, 'fear': 0, 'nervousness': 2, 'sadness': 0, 'suffering': 0, 'shame': 2}\n",
      "\n",
      "Teks: 'My mistake from yesterday still causes me a lot of shame.'\n",
      "Prediksi: {'anxiety': 0, 'fear': 0, 'nervousness': 2, 'sadness': 1, 'suffering': 0, 'shame': 2}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Fungsi untuk Prediksi Teks Baru (di luar dataset) ---\n",
    "def predict_emotion_for_new_text(text, model, tokenizer, maxlen, emotion_labels):\n",
    "    processed_text = preprocess_text(text) # Gunakan preprocessing yang sama\n",
    "    seq = tokenizer.texts_to_sequences([processed_text])\n",
    "    padded_seq = pad_sequences(seq, maxlen=maxlen, padding='post', truncating='post')\n",
    "\n",
    "    raw_prediction = model.predict(padded_seq, verbose=0)[0]\n",
    "\n",
    "    # Pasca-pemrosesan untuk mendapatkan 3 emosi dominan dalam level\n",
    "    predicted_dict = top_3_level_emotions(\n",
    "        {label: score for label, score in zip(emotion_labels, raw_prediction)}\n",
    "    )\n",
    "    return predicted_dict\n",
    "\n",
    "print(\"\\n--- Contoh Prediksi untuk Teks Baru ---\")\n",
    "new_texts = [\n",
    "    \"I am truly happy today, life is great!\",\n",
    "    \"This situation makes me feel so helpless and trapped.\",\n",
    "    \"I have an exam soon, feeling a mix of nerves and excitement.\",\n",
    "    \"The news today filled me with deep sorrow.\",\n",
    "    \"I endured a lot, but I am recovering.\",\n",
    "    \"My mistake from yesterday still causes me a lot of shame.\"\n",
    "]\n",
    "\n",
    "for text in new_texts:\n",
    "    predicted_new_text = predict_emotion_for_new_text(text, best_model, tokenizer, maxlen, emotion_labels)\n",
    "    print(f\"Teks: '{text}'\")\n",
    "    print(f\"Prediksi: {predicted_new_text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f54a89ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pastikan deep_translator sudah terinstal: pip install deep-translator\n",
    "from deep_translator import GoogleTranslator # Import library GoogleTranslator\n",
    "import tensorflow as tf # Pastikan TensorFlow sudah terinstal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1cf464e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Konversi Model ke TensorFlow Lite ---\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\laila\\AppData\\Local\\Temp\\tmp95br8jzj\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\laila\\AppData\\Local\\Temp\\tmp95br8jzj\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\laila\\AppData\\Local\\Temp\\tmp95br8jzj'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 100), dtype=tf.float32, name='input_layer_1')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 6), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2094224563600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2094224561872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2094224567632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2094224561488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2094224568976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2094224568784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2094224564368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2094224563792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2094224564176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2094224562640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2094224569744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2094224561680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2094224563408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2094224569168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2094224562448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2094224563216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2094224563984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "Model TensorFlow Lite berhasil disimpan di: emotion_regression_model_v2.tflite\n"
     ]
    }
   ],
   "source": [
    "# Muat kembali model terbaik jika belum di-load\n",
    "try:\n",
    "    best_model = tf.keras.models.load_model('basic_emotion_regression_model.h5', compile=False)\n",
    "    best_model.compile(loss=tf.keras.losses.MeanSquaredError(), optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), metrics=[tf.keras.metrics.MeanSquaredError(), 'mae'])\n",
    "except Exception as e:\n",
    "    print(f\"Gagal memuat model terbaik, menggunakan model yang terakhir dilatih: {e}\")\n",
    "    best_model = model\n",
    "\n",
    "print(\"\\n--- Konversi Model ke TensorFlow Lite ---\")\n",
    "# Buat TFLite converter\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(best_model)\n",
    "\n",
    "# --- Tambahkan konfigurasi ini untuk mengatasi ConverterError ---\n",
    "converter.target_spec.supported_ops = [\n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS,  # Mengizinkan operasi built-in TFLite\n",
    "    tf.lite.OpsSet.SELECT_TF_OPS    # Mengizinkan operasi TensorFlow yang tidak memiliki built-in TFLite\n",
    "]\n",
    "# Ini adalah flag eksperimental yang disarankan untuk dinonaktifkan\n",
    "converter._experimental_lower_tensor_list_ops = False\n",
    "# Opsional: Jika Anda ingin mengizinkan operasi TF yang tidak standar, bisa juga tambahkan:\n",
    "# converter.allow_custom_ops = True\n",
    "\n",
    "# Konversi model\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Simpan model TFLite ke file\n",
    "tflite_model_path = 'emotion_regression_model_v2.tflite'\n",
    "with open(tflite_model_path, 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "print(f\"Model TensorFlow Lite berhasil disimpan di: {tflite_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "14e78c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Fungsi untuk Prediksi Menggunakan Model TFLite dengan Terjemahan ---\n",
    "def predict_emotion_tflite_with_translation(text, tflite_model_path, tokenizer, maxlen, emotion_labels):\n",
    "    # Inisialisasi interpreter TFLite\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    # Dapatkan detail input dan output\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    # 1. Terjemahkan teks jika bukan bahasa Inggris\n",
    "    # Anda mungkin perlu logika deteksi bahasa di sini,\n",
    "    # atau secara eksplisit meminta pengguna memilih bahasa input.\n",
    "    # Untuk contoh ini, kita asumsikan input bisa Bahasa Indonesia dan langsung diterjemahkan.\n",
    "    try:\n",
    "        # Menentukan source language secara otomatis atau eksplisit 'id'\n",
    "        # Target language adalah 'en' (English)\n",
    "        translated_text = GoogleTranslator(source='auto', target='en').translate(text)\n",
    "        print(f\"Teks asli: '{text}' -> Diterjemahkan: '{translated_text}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"Gagal menerjemahkan teks: {e}. Menggunakan teks asli.\")\n",
    "        translated_text = text # Fallback ke teks asli jika terjemahan gagal\n",
    "\n",
    "    # 2. Pra-proses teks yang sudah diterjemahkan (atau teks asli jika gagal diterjemahkan)\n",
    "    processed_text = preprocess_text(translated_text) # Gunakan preprocessing yang sama\n",
    "\n",
    "     # 3. Tokenisasi dan Padding\n",
    "    seq = tokenizer.texts_to_sequences([processed_text])\n",
    "    padded_seq = tf.keras.preprocessing.sequence.pad_sequences(seq, maxlen=maxlen, padding='post', truncating='post')\n",
    "\n",
    "    # --- PERUBAHAN DI SINI ---\n",
    "    # Convert input data to the type expected by the TFLite model\n",
    "    # Ganti dtype=np.int32 menjadi dtype=np.float32\n",
    "    input_data = np.array(padded_seq, dtype=np.float32) # <-- Perubahan ini\n",
    "\n",
    "    # 4. Set input tensor dan jalankan inferensi\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # 5. Dapatkan hasil prediksi\n",
    "    raw_prediction = interpreter.get_tensor(output_details[0]['index'])[0]\n",
    "\n",
    "    # 6. Pasca-pemrosesan untuk mendapatkan 3 emosi dominan dalam level (fungsi yang sama dari Keras model)\n",
    "    predicted_dict = top_3_level_emotions(\n",
    "        {label: score for label, score in zip(emotion_labels, raw_prediction)}\n",
    "    )\n",
    "    return predicted_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "807fbbc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Contoh Prediksi untuk Teks Baru (dengan TFLite dan Terjemahan) ---\n",
      "Teks asli: 'Saya sangat bahagia hari ini, hidup ini indah!' -> Diterjemahkan: 'I am very happy today, life is beautiful!'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\eng-model\\engmod\\Lib\\site-packages\\tensorflow\\lite\\python\\interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
      "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
      "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
      "    for details.\n",
      "    \n",
      "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediksi: {'anxiety': 0, 'fear': 1, 'nervousness': 2, 'sadness': 0, 'suffering': 0, 'shame': 2}\n",
      "\n",
      "Teks asli: 'Situasi ini membuat saya merasa sangat tidak berdaya dan terperangkap.' -> Diterjemahkan: 'This situation made me feel very helpless and trapped.'\n",
      "Prediksi: {'anxiety': 0, 'fear': 0, 'nervousness': 2, 'sadness': 0, 'suffering': 2, 'shame': 2}\n",
      "\n",
      "Teks asli: 'Saya akan ujian sebentar lagi, merasa gugup sekaligus bersemangat.' -> Diterjemahkan: 'I will take an exam soon, feel nervous and excited.'\n",
      "Prediksi: {'anxiety': 2, 'fear': 0, 'nervousness': 3, 'sadness': 0, 'suffering': 0, 'shame': 2}\n",
      "\n",
      "Teks asli: 'Berita hari ini membuat saya sedih sekali.' -> Diterjemahkan: 'Today's news makes me so sad.'\n",
      "Prediksi: {'anxiety': 0, 'fear': 0, 'nervousness': 2, 'sadness': 2, 'suffering': 0, 'shame': 2}\n",
      "\n",
      "Teks asli: 'Saya sudah banyak menderita, tapi saya pulih.' -> Diterjemahkan: 'I have suffered a lot, but I recovered.'\n",
      "Prediksi: {'anxiety': 0, 'fear': 0, 'nervousness': 2, 'sadness': 0, 'suffering': 2, 'shame': 2}\n",
      "\n",
      "Teks asli: 'Kesalahan saya kemarin masih menyebabkan banyak rasa malu.' -> Diterjemahkan: 'My mistake yesterday still caused a lot of shame.'\n",
      "Prediksi: {'anxiety': 0, 'fear': 0, 'nervousness': 2, 'sadness': 1, 'suffering': 0, 'shame': 2}\n",
      "\n",
      "Teks asli: 'I am very worried about my health.' -> Diterjemahkan: 'I am very worried about my health.'\n",
      "Prediksi: {'anxiety': 0, 'fear': 0, 'nervousness': 2, 'sadness': 0, 'suffering': 1, 'shame': 2}\n",
      "\n",
      "Teks asli: 'Saya merasa cemas dengan masa depan.' -> Diterjemahkan: 'I feel worried about the future.'\n",
      "Prediksi: {'anxiety': 1, 'fear': 0, 'nervousness': 2, 'sadness': 0, 'suffering': 0, 'shame': 2}\n",
      "\n",
      "Teks asli: 'Sungguh menyedihkan apa yang terjadi kemarin.' -> Diterjemahkan: 'It's sad what happened yesterday.'\n",
      "Prediksi: {'anxiety': 0, 'fear': 0, 'nervousness': 2, 'sadness': 2, 'suffering': 0, 'shame': 2}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Contoh Penggunaan Prediksi dengan TFLite dan Terjemahan ---\n",
    "print(\"\\n--- Contoh Prediksi untuk Teks Baru (dengan TFLite dan Terjemahan) ---\")\n",
    "new_texts_with_id = [\n",
    "    \"Saya sangat bahagia hari ini, hidup ini indah!\", # Indonesian\n",
    "    \"Situasi ini membuat saya merasa sangat tidak berdaya dan terperangkap.\", # Indonesian\n",
    "    \"Saya akan ujian sebentar lagi, merasa gugup sekaligus bersemangat.\", # Indonesian\n",
    "    \"Berita hari ini membuat saya sedih sekali.\", # Indonesian\n",
    "    \"Saya sudah banyak menderita, tapi saya pulih.\", # Indonesian\n",
    "    \"Kesalahan saya kemarin masih menyebabkan banyak rasa malu.\", # Indonesian\n",
    "    \"I am very worried about my health.\", # English (to show it still works for English)\n",
    "    \"Saya merasa cemas dengan masa depan.\", # Indonesian\n",
    "    \"Sungguh menyedihkan apa yang terjadi kemarin.\" # Indonesian\n",
    "]\n",
    "\n",
    "# Ambil best_model dari proses sebelumnya (yang sudah di-load atau fall-back)\n",
    "# emotion_labels dan maxlen juga dari definisi sebelumnya\n",
    "\n",
    "for text in new_texts_with_id:\n",
    "    predicted_emotion = predict_emotion_tflite_with_translation(text, tflite_model_path, tokenizer, maxlen, emotion_labels)\n",
    "    print(f\"Prediksi: {predicted_emotion}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "engmod",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
