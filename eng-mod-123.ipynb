{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1801e5d4",
   "metadata": {},
   "source": [
    "# 1. Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94a4c419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import libraries (sama seperti sebelumnya)\n",
    "from empath import Empath\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import contractions\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense, Dropout, Layer\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix, hamming_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5488011",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\laila\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\laila\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\laila\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Inisialisasi\n",
    "lexicon = Empath()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "custom_stopwords = {'like', 'get', 'go', 'know', 'would', 'could', 'also'}\n",
    "stop_words.update(custom_stopwords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea89fb2",
   "metadata": {},
   "source": [
    "# 2. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9332b07a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>statement</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Leaves are also standby in front of the PC ......</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bismillah for Eid 2021, you get a few question...</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I want to spend a lot of time shopping for sna...</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I like to be grateful, don't you think, if you...</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>why is this person blg parcel hampers blah bla...</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           statement   label\n",
       "0  Leaves are also standby in front of the PC ......  Normal\n",
       "1  Bismillah for Eid 2021, you get a few question...  Normal\n",
       "2  I want to spend a lot of time shopping for sna...  Normal\n",
       "3  I like to be grateful, don't you think, if you...  Normal\n",
       "4  why is this person blg parcel hampers blah bla...  Normal"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"filtered.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d23a8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10299 entries, 0 to 10298\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   statement  10299 non-null  object\n",
      " 1   label      10299 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 161.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfb10b8",
   "metadata": {},
   "source": [
    "# 3. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e52e6a7",
   "metadata": {},
   "source": [
    "## 3.1. Cleaning n lemmetizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5882ad0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()  # Mengubah teks menjadi huruf kecil \n",
    "    text = contractions.fix(text)  # Memperbaiki kontraksi \n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)  # Menghapus URL \n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)  # Menghapus karakter non-ASCII \n",
    "    text = re.sub(r'\\d+', '', text)  # Menghapus angka \n",
    "    text = re.sub(rf\"[{re.escape(string.punctuation)}]\", '', text)  # Menghapus tanda baca \n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Ganti multiple spasi dengan 1 spasi)\n",
    "    return text  # Mengembalikan teks yang sudah dibersihkan\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = clean_text(text) \n",
    "    words = text.split()  # Memisahkan teks menjadi list kata \n",
    "    processed_words = []  # List untuk menyimpan kata yang sudah diproses\n",
    "    for word in words:\n",
    "        if word not in stop_words and len(word) > 2:  # Filter: hapus stopword dan kata dengan panjang ≤ 2\n",
    "            lemma = lemmatizer.lemmatize(word, pos='v')  # Lemmatisasi sebagai verb (e.g., \"running\" → \"run\")\n",
    "            lemma = lemmatizer.lemmatize(lemma, pos='n')  # Lemmatisasi sebagai noun (e.g., \"wolves\" → \"wolf\")\n",
    "            lemma = lemmatizer.lemmatize(lemma, pos='a')  # Lemmatisasi sebagai adjective (e.g., \"better\" → \"good\")\n",
    "            lemma = lemmatizer.lemmatize(lemma, pos='r')  # Lemmatisasi sebagai adverb (e.g., \"quickly\" → \"quick\")\n",
    "            processed_words.append(lemma)  # Tambahkan kata yang sudah dilematisasi ke list\n",
    "    return ' '.join(processed_words)  # Gabungkan list kata menjadi teks dengan spasi\n",
    "\n",
    "df['statement'] = df['statement'].astype(str)\n",
    "df['cleaned_statement'] = df['statement'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b38b0d4",
   "metadata": {},
   "source": [
    "## 3.2. Emotion Extraction with Emapth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1eb3529f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Daftar emosi dari Empath yang relevan\n",
    "emotions = ['anxiety', 'fear', 'nervousness', 'sadness', 'suffering', 'shame']\n",
    "\n",
    "#Analisis teks menggunakan lexicon Empath, dengan kategori yang telah didefinisikan \n",
    "def label_from_empath(text):\n",
    "    scores = lexicon.analyze(text, categories=emotions, normalize=True)\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1982d718",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mendefinisikan keywords emosi yang tidak terdeteksi Empath\n",
    "emotion_keywords = [\n",
    "    'sleep', 'restless', 'panic', 'worried', 'scared',\n",
    "    'cry', 'sad', 'guilt', 'confused', 'fear',\n",
    "    'dizzy', 'pressure', 'tired', 'alone', 'anxious', 'hopeless', 'worthless', 'suicidal', \n",
    "    'overwhelmed', 'isolated', 'numb', 'empty',\n",
    "    'heartbroken', 'misery', 'despair', 'regret'\n",
    "]\n",
    "\n",
    "#Fungsi cek keywords\n",
    "def contains_emotion_keyword(text):\n",
    "    return any(re.search(rf'\\b{kw}\\b', text.lower()) for kw in emotion_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f336995",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set fallback\n",
    "def top_n_emotions_with_fallback(score_dict, text, label, n=3):\n",
    "    #Jika label normal, fallback ke emosi neutral\n",
    "    if label.lower() == 'normal':\n",
    "        return ['neutral']\n",
    "    \n",
    "    #Jika bukan dictionary atau jika kosong, langsung fallback ke anxiety\n",
    "    if not isinstance(score_dict, dict) or not score_dict:\n",
    "        return ['anxiety']  \n",
    "\n",
    "    sorted_items = sorted(score_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    top = [emotion for emotion, score in sorted_items[:n] if score > 0]\n",
    "\n",
    "    #Jika tidak ada emosi dengan skor positif, fallback ke anxiety saja\n",
    "    return top if top else ['anxiety']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aca235c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Membuat empath_score dari cleaned_statement\n",
    "df['empath_scores'] = df['cleaned_statement'].apply(label_from_empath)\n",
    "\n",
    "#Membuat top_emotion berdasarkan empath_score tertinggi dengan menerapkan fallback jika skor kurang\n",
    "df['top_emotions'] = df.apply(\n",
    "    lambda row: top_n_emotions_with_fallback(\n",
    "        row['empath_scores'], row['cleaned_statement'], row['label']\n",
    "    ), axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6dc339",
   "metadata": {},
   "source": [
    "# 4. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61e319b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hapus None dari kolom top_emotion\n",
    "df['top_emotions'] = df['top_emotions'].apply(lambda x: list(filter(None, x)))  # hapus None\n",
    "#Tokenisasi teks\n",
    "MAX_NUM_WORDS = 10000\n",
    "MAX_SEQ_LENGTH = 100\n",
    "\n",
    "#Membuat tokenizer untuk mengubah teks menjadi token numerik, dengan handling untuk kata-kata yang tidak ada dalam vocab\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, oov_token='<OOV>')\n",
    "df['cleaned_statement'] = df['cleaned_statement'].astype(str)\n",
    "#Melatih tokenizer pada teks agar bisa mengenali kata-kata dan membuat mapping numerik\n",
    "tokenizer.fit_on_texts(df['cleaned_statement'])\n",
    "sequences = tokenizer.texts_to_sequences(df['cleaned_statement'])\n",
    "\n",
    "#Menambahkan padding agar setiap sequence memiliki panjang yang seragam\n",
    "padded_sequences = pad_sequences(sequences, maxlen=MAX_SEQ_LENGTH, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b873b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pastikan list emosi tidak mengandung NaN/None dan semua elemennya string\n",
    "def clean_emotion_list(emotion_list):\n",
    "    if not isinstance(emotion_list, list):\n",
    "        return []\n",
    "    return [str(emotion) for emotion in emotion_list if pd.notnull(emotion)]\n",
    "\n",
    "df['top_emotions'] = df['top_emotions'].apply(clean_emotion_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fd268d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MultiLabel Binarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(df['top_emotions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c41197bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(padded_sequences, y, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "68251f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\eng-model\\engmod\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "d:\\eng-model\\engmod\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:86: UserWarning: Argument `decay` is no longer supported and will be ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_2 (\u001b[38;5;33mBidirectional\u001b[0m) │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_3 (\u001b[38;5;33mBidirectional\u001b[0m) │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=MAX_NUM_WORDS, output_dim=128, input_length=MAX_SEQ_LENGTH))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True)))  # LSTM pertama\n",
    "model.add(Bidirectional(LSTM(32, return_sequences=False)))  # LSTM kedua untuk informasi lebih dalam\n",
    "model.add(Dropout(0.3))  # Mengurangi dropout agar tidak kehilangan terlalu banyak informasi\n",
    "model.add(Dense(len(mlb.classes_), activation='sigmoid'))  # Aktivasi sigmoid karena multi-label\n",
    "\n",
    "# Compile Model\n",
    "optimizer = Adam(learning_rate=1e-3, decay=1e-5)  # Menggunakan decay untuk stabilitas\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# Summary Model\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d43cc665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 162ms/step - accuracy: 0.5459 - loss: 0.4604 - val_accuracy: 0.6291 - val_loss: 0.2664\n",
      "Epoch 2/20\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 150ms/step - accuracy: 0.6527 - loss: 0.2673 - val_accuracy: 0.6417 - val_loss: 0.2425\n",
      "Epoch 3/20\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 150ms/step - accuracy: 0.6672 - loss: 0.2346 - val_accuracy: 0.6456 - val_loss: 0.2189\n",
      "Epoch 4/20\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 150ms/step - accuracy: 0.7114 - loss: 0.2041 - val_accuracy: 0.7437 - val_loss: 0.2033\n",
      "Epoch 5/20\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 151ms/step - accuracy: 0.7361 - loss: 0.1748 - val_accuracy: 0.8117 - val_loss: 0.1917\n",
      "Epoch 6/20\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 151ms/step - accuracy: 0.7567 - loss: 0.1593 - val_accuracy: 0.7903 - val_loss: 0.1881\n",
      "Epoch 7/20\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 151ms/step - accuracy: 0.7480 - loss: 0.1473 - val_accuracy: 0.7534 - val_loss: 0.1781\n",
      "Epoch 8/20\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 149ms/step - accuracy: 0.7484 - loss: 0.1263 - val_accuracy: 0.7350 - val_loss: 0.1792\n",
      "Epoch 9/20\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 150ms/step - accuracy: 0.7501 - loss: 0.1089 - val_accuracy: 0.7214 - val_loss: 0.1683\n",
      "Epoch 10/20\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 150ms/step - accuracy: 0.7269 - loss: 0.1002 - val_accuracy: 0.7049 - val_loss: 0.1692\n",
      "Epoch 11/20\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 151ms/step - accuracy: 0.7272 - loss: 0.0902 - val_accuracy: 0.6641 - val_loss: 0.1694\n",
      "Epoch 12/20\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 152ms/step - accuracy: 0.7218 - loss: 0.0820 - val_accuracy: 0.6854 - val_loss: 0.1752\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stop]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7bdfde82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 64ms/step\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     anxiety       0.45      0.28      0.35        32\n",
      "        fear       0.82      0.87      0.85       266\n",
      " nervousness       0.89      0.94      0.92       368\n",
      "     neutral       0.95      0.94      0.94       568\n",
      "     sadness       0.82      0.79      0.81       233\n",
      "       shame       0.83      0.83      0.83       204\n",
      "   suffering       0.73      0.55      0.63       120\n",
      "\n",
      "   micro avg       0.87      0.86      0.86      1791\n",
      "   macro avg       0.78      0.74      0.76      1791\n",
      "weighted avg       0.86      0.86      0.86      1791\n",
      " samples avg       0.88      0.87      0.87      1791\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\eng-model\\engmod\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_bin = (y_pred > 0.5).astype(int)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_bin, target_names=mlb.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b76b645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_top3_emotions(text, tokenizer, model, mlb, max_len=100):\n",
    "#     # Lowercase & konversi teks\n",
    "#     text = str(text).lower()\n",
    "\n",
    "#     # Tokenisasi & padding\n",
    "#     seq = tokenizer.texts_to_sequences([text])\n",
    "#     padded = pad_sequences(seq, maxlen=max_len, padding='post')\n",
    "\n",
    "#     # Prediksi\n",
    "#     pred = model.predict(padded)[0]\n",
    "\n",
    "#     # Ambil 3 skor tertinggi\n",
    "#     top3_indices = np.argsort(pred)[-3:][::-1]\n",
    "#     top3_labels = [mlb.classes_[i] for i in top3_indices]\n",
    "#     top3_scores = [round(pred[i], 4) for i in top3_indices]\n",
    "\n",
    "#     return list(zip(top3_labels, top3_scores))  # Hasil: [(label1, score1), ...]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "46df4fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_top3_emotions(text, tokenizer, model, mlb, max_len=100):\n",
    "    text = preprocess_text(text)  # Terapkan preprocessing sebelum tokenisasi\n",
    "    seq = tokenizer.texts_to_sequences([text])  # Tokenisasi teks yang sudah dibersihkan\n",
    "    padded = pad_sequences(seq, maxlen=max_len, padding='post')  # Padding untuk panjang seragam\n",
    "    \n",
    "    pred = model.predict(padded)[0]  # Prediksi dengan model\n",
    "    top3_indices = np.argsort(pred)[-3:][::-1]  # Ambil 3 skor tertinggi\n",
    "    top3_labels = [mlb.classes_[i] for i in top3_indices]  # Label emosi teratas\n",
    "    top3_scores = [round(pred[i], 4) for i in top3_indices]  # Skor prediksi\n",
    "    \n",
    "    return list(zip(top3_labels, top3_scores))  # Format output: [(label1, score1), ...]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bdf49c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "Teks Asli: Anxiety AFTER dental work I had a tooth removed today and my anxiety is super high. Not sure why..I am taking Advi for pain and Lexapro as normal..I don't like how I feel right now though so keyed up, nausea, etc.\n",
      "Teks Setelah Preprocessing: anxiety dental work tooth remove today anxiety super high sure whyi take advi pain lexapro normali feel right though key nausea etc\n",
      "Top 3 Emosi dan Skor:\n",
      "- fear: 0.9725000262260437\n",
      "- nervousness: 0.9685999751091003\n",
      "- sadness: 0.9358000159263611\n"
     ]
    }
   ],
   "source": [
    "new_text = \"Anxiety AFTER dental work I had a tooth removed today and my anxiety is super high. Not sure why..I am taking Advi for pain and Lexapro as normal..I don't like how I feel right now though so keyed up, nausea, etc.\"\n",
    "\n",
    "# Terapkan preprocessing sebelum inference\n",
    "cleaned_text = preprocess_text(new_text)\n",
    "\n",
    "# Prediksi emosi berdasarkan teks yang sudah dibersihkan\n",
    "top3 = predict_top3_emotions(cleaned_text, tokenizer, model, mlb)\n",
    "\n",
    "# Cetak hasil teks setelah preprocessing dan prediksi emosi\n",
    "print(\"Teks Asli:\", new_text)\n",
    "print(\"Teks Setelah Preprocessing:\", cleaned_text)\n",
    "print(\"Top 3 Emosi dan Skor:\")\n",
    "for label, score in top3:\n",
    "    print(f\"- {label}: {score}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "engmod",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
