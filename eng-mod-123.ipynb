{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1801e5d4",
   "metadata": {},
   "source": [
    "# 1. Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94a4c419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import libraries (sama seperti sebelumnya)\n",
    "from empath import Empath\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import contractions\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense, Dropout, Layer\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix, hamming_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c5488011",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\laila\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\laila\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\laila\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Inisialisasi\n",
    "lexicon = Empath()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "custom_stopwords = {'like', 'get', 'go', 'know', 'would', 'could', 'also'}\n",
    "stop_words.update(custom_stopwords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea89fb2",
   "metadata": {},
   "source": [
    "# 2. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9332b07a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>statement</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Leaves are also standby in front of the PC ......</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bismillah for Eid 2021, you get a few question...</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I want to spend a lot of time shopping for sna...</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I like to be grateful, don't you think, if you...</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>why is this person blg parcel hampers blah bla...</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           statement   label\n",
       "0  Leaves are also standby in front of the PC ......  Normal\n",
       "1  Bismillah for Eid 2021, you get a few question...  Normal\n",
       "2  I want to spend a lot of time shopping for sna...  Normal\n",
       "3  I like to be grateful, don't you think, if you...  Normal\n",
       "4  why is this person blg parcel hampers blah bla...  Normal"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"filtered.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d23a8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10299 entries, 0 to 10298\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   statement  10299 non-null  object\n",
      " 1   label      10299 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 161.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfb10b8",
   "metadata": {},
   "source": [
    "# 3. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e52e6a7",
   "metadata": {},
   "source": [
    "## 3.1 Cleaning n lemmetizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5882ad0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()  # Mengubah teks menjadi huruf kecil \n",
    "    text = contractions.fix(text)  # Memperbaiki kontraksi \n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)  # Menghapus URL \n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)  # Menghapus karakter non-ASCII \n",
    "    text = re.sub(r'\\d+', '', text)  # Menghapus angka \n",
    "    text = re.sub(rf\"[{re.escape(string.punctuation)}]\", '', text)  # Menghapus tanda baca \n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Ganti multiple spasi dengan 1 spasi)\n",
    "    return text  # Mengembalikan teks yang sudah dibersihkan\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = clean_text(text) \n",
    "    words = text.split()  # Memisahkan teks menjadi list kata \n",
    "    processed_words = []  # List untuk menyimpan kata yang sudah diproses\n",
    "    for word in words:\n",
    "        if word not in stop_words and len(word) > 2:  # Filter: hapus stopword dan kata dengan panjang ≤ 2\n",
    "            lemma = lemmatizer.lemmatize(word, pos='v')  # Lemmatisasi sebagai verb (e.g., \"running\" → \"run\")\n",
    "            lemma = lemmatizer.lemmatize(lemma, pos='n')  # Lemmatisasi sebagai noun (e.g., \"wolves\" → \"wolf\")\n",
    "            lemma = lemmatizer.lemmatize(lemma, pos='a')  # Lemmatisasi sebagai adjective (e.g., \"better\" → \"good\")\n",
    "            lemma = lemmatizer.lemmatize(lemma, pos='r')  # Lemmatisasi sebagai adverb (e.g., \"quickly\" → \"quick\")\n",
    "            processed_words.append(lemma)  # Tambahkan kata yang sudah dilematisasi ke list\n",
    "    return ' '.join(processed_words)  # Gabungkan list kata menjadi teks dengan spasi\n",
    "\n",
    "df['statement'] = df['statement'].astype(str)\n",
    "df['cleaned_statement'] = df['statement'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1eb3529f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Daftar emosi dari Empath yang relevan\n",
    "emotions = ['anxiety', 'fear', 'nervousness', 'sadness', 'suffering', 'shame']\n",
    "\n",
    "#Analisis teks menggunakan lexicon Empath, dengan kategori yang telah didefinisikan \n",
    "def label_from_empath(text):\n",
    "    scores = lexicon.analyze(text, categories=emotions, normalize=True)\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1982d718",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_keywords = [\n",
    "    'sleep', 'restless', 'panic', 'worried', 'scared',\n",
    "    'cry', 'sad', 'guilt', 'confused', 'fear',\n",
    "    'dizzy', 'pressure', 'tired', 'alone', 'anxious', 'hopeless', 'worthless', 'suicidal', \n",
    "    'overwhelmed', 'isolated', 'numb', 'empty',\n",
    "    'heartbroken', 'misery', 'despair', 'regret'\n",
    "]\n",
    "\n",
    "def contains_emotion_keyword(text):\n",
    "    return any(re.search(rf'\\b{kw}\\b', text.lower()) for kw in emotion_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4f336995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_emotions_with_fallback(score_dict, text, label, n=3):\n",
    "    if label.lower() == 'normal':\n",
    "        return ['neutral']\n",
    "    \n",
    "    # Jika bukan dictionary atau jika kosong, langsung fallback ke 'anxiety'\n",
    "    if not isinstance(score_dict, dict) or not score_dict:\n",
    "        return ['anxiety']  \n",
    "\n",
    "    sorted_items = sorted(score_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    top = [emotion for emotion, score in sorted_items[:n] if score > 0]\n",
    "\n",
    "    # Jika tidak ada emosi dengan skor positif, fallback ke 'anxiety' saja\n",
    "    return top if top else ['anxiety']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aca235c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['empath_scores'] = df['cleaned_statement'].apply(label_from_empath)\n",
    "\n",
    "df['top_emotions'] = df.apply(\n",
    "    lambda row: top_n_emotions_with_fallback(\n",
    "        row['empath_scores'], row['cleaned_statement'], row['label']\n",
    "    ), axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "afd45308",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_top_emotions_separate(score_dict, text, label, n=3):\n",
    "    \"\"\"\n",
    "    Extract top 3 emotions ke kolom terpisah\n",
    "    \"\"\"\n",
    "    if label.lower() == 'normal':\n",
    "        return ['neutral', None, None]\n",
    "    \n",
    "    # Jika bukan dictionary atau kosong, langsung fallback ke 'anxiety'\n",
    "    if not isinstance(score_dict, dict) or not score_dict:\n",
    "        return ['anxiety', None, None]\n",
    "    \n",
    "    sorted_items = sorted(score_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_emotions = [emotion for emotion, score in sorted_items[:n] if score > 0]\n",
    "    \n",
    "    # Jika tidak ada emosi dengan skor positif, fallback ke 'anxiety'\n",
    "    if not top_emotions:\n",
    "        return ['anxiety', None, None]\n",
    "    \n",
    "    # Pad dengan None jika kurang dari 3\n",
    "    while len(top_emotions) < 3:\n",
    "        top_emotions.append(None)\n",
    "    \n",
    "    return top_emotions[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "822d2d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_emotions_list = df.apply(\n",
    "    lambda row: extract_top_emotions_separate(\n",
    "        row['empath_scores'], row['cleaned_statement'], row['label']\n",
    "    ), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5588ac47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data with separate emotion columns:\n",
      "                                           statement   label    top_1 top_2  \\\n",
      "0  Leaves are also standby in front of the PC ......  Normal  neutral  None   \n",
      "1  Bismillah for Eid 2021, you get a few question...  Normal  neutral  None   \n",
      "2  I want to spend a lot of time shopping for sna...  Normal  neutral  None   \n",
      "3  I like to be grateful, don't you think, if you...  Normal  neutral  None   \n",
      "4  why is this person blg parcel hampers blah bla...  Normal  neutral  None   \n",
      "5  I want to take a day off from work and then ta...  Normal  neutral  None   \n",
      "6  How many bbl tickets are now? How come I want ...  Normal  neutral  None   \n",
      "7  I bought a shirt for 200, but I've never worn ...  Normal  neutral  None   \n",
      "8  Chinese foreigners can enter at will because t...  Normal  neutral  None   \n",
      "9  09.35 WIB #Tol_JORR_E TMII - Cikunir - Cakung ...  Normal  neutral  None   \n",
      "\n",
      "  top_3  \n",
      "0  None  \n",
      "1  None  \n",
      "2  None  \n",
      "3  None  \n",
      "4  None  \n",
      "5  None  \n",
      "6  None  \n",
      "7  None  \n",
      "8  None  \n",
      "9  None  \n"
     ]
    }
   ],
   "source": [
    "df['top_1'] = [emotions[0] for emotions in top_emotions_list]\n",
    "df['top_2'] = [emotions[1] for emotions in top_emotions_list]\n",
    "df['top_3'] = [emotions[2] for emotions in top_emotions_list]\n",
    "\n",
    "print(\"Sample data with separate emotion columns:\")\n",
    "print(df[['statement', 'label', 'top_1', 'top_2', 'top_3']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e41990da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"top123-2.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "engmod",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
