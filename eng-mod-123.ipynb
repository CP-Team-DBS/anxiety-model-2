{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1801e5d4",
   "metadata": {},
   "source": [
    "# 1. Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94a4c419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import libraries (sama seperti sebelumnya)\n",
    "from empath import Empath\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import contractions\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense, Dropout, Layer\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix, hamming_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5488011",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\laila\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\laila\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\laila\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Inisialisasi\n",
    "lexicon = Empath()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "custom_stopwords = {'like', 'get', 'go', 'know', 'would', 'could', 'also'}\n",
    "stop_words.update(custom_stopwords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea89fb2",
   "metadata": {},
   "source": [
    "# 2. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9332b07a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>statement</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Leaves are also standby in front of the PC ......</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bismillah for Eid 2021, you get a few question...</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I want to spend a lot of time shopping for sna...</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I like to be grateful, don't you think, if you...</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>why is this person blg parcel hampers blah bla...</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           statement   label\n",
       "0  Leaves are also standby in front of the PC ......  Normal\n",
       "1  Bismillah for Eid 2021, you get a few question...  Normal\n",
       "2  I want to spend a lot of time shopping for sna...  Normal\n",
       "3  I like to be grateful, don't you think, if you...  Normal\n",
       "4  why is this person blg parcel hampers blah bla...  Normal"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"datasets.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d23a8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11053 entries, 0 to 11052\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   statement  11053 non-null  object\n",
      " 1   label      11053 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 172.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfb10b8",
   "metadata": {},
   "source": [
    "# 3. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e52e6a7",
   "metadata": {},
   "source": [
    "## 3.1 Cleaning n lemmetizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5882ad0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = contractions.fix(text)\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(rf\"[{re.escape(string.punctuation)}]\", '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = clean_text(text)\n",
    "    words = text.split()\n",
    "    processed_words = []\n",
    "    for word in words:\n",
    "        if word not in stop_words and len(word) > 2:\n",
    "            lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "            lemma = lemmatizer.lemmatize(lemma, pos='n')\n",
    "            lemma = lemmatizer.lemmatize(lemma, pos='a')\n",
    "            lemma = lemmatizer.lemmatize(lemma, pos='r')\n",
    "            processed_words.append(lemma)\n",
    "    return ' '.join(processed_words)\n",
    "\n",
    "df['statement'] = df['statement'].astype(str)\n",
    "df['cleaned_statement'] = df['statement'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb3529f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Daftar emosi dari Empath yang relevan\n",
    "emotions = ['anxiety', 'fear', 'nervousness', 'sadness', 'suffering', 'shame']\n",
    "\n",
    "def label_from_empath(text):\n",
    "    scores = lexicon.analyze(text, categories=emotions, normalize=True)\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1982d718",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_keywords = [\n",
    "    'sleep', 'restless', 'panic', 'worried', 'scared',\n",
    "    'cry', 'sad', 'guilt', 'confused', 'fear',\n",
    "    'dizzy', 'pressure', 'tired', 'alone'\n",
    "]\n",
    "\n",
    "def contains_emotion_keyword(text):\n",
    "    return any(re.search(rf'\\b{kw}\\b', text.lower()) for kw in emotion_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f336995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_emotions_with_fallback(score_dict, text, label, n=3):\n",
    "    if label.lower() == 'normal':\n",
    "        return ['neutral']\n",
    "    if not isinstance(score_dict, dict) or sum(score_dict.values()) < 0.05:\n",
    "        if contains_emotion_keyword(text):\n",
    "            return ['anxiety']  # fallback default jika keywords ada\n",
    "        return ['neutral']\n",
    "    sorted_items = sorted(score_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    top = [emotion for emotion, score in sorted_items[:n] if score > 0]\n",
    "    return top if top else ['neutral']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aca235c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['empath_scores'] = df['cleaned_statement'].apply(label_from_empath)\n",
    "\n",
    "df['top_emotions'] = df.apply(\n",
    "    lambda row: top_n_emotions_with_fallback(\n",
    "        row['empath_scores'], row['cleaned_statement'], row['label']\n",
    "    ), axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afd45308",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_top_emotions_separate(score_dict, text, label, n=3):\n",
    "    \"\"\"\n",
    "    Extract top 3 emotions ke kolom terpisah\n",
    "    \"\"\"\n",
    "    if label.lower() == 'normal':\n",
    "        return ['neutral', None, None]\n",
    "    \n",
    "    if not isinstance(score_dict, dict) or sum(score_dict.values()) < 0.05:\n",
    "        if contains_emotion_keyword(text):\n",
    "            return ['anxiety', None, None]\n",
    "        return ['neutral', None, None]\n",
    "    \n",
    "    sorted_items = sorted(score_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_emotions = [emotion for emotion, score in sorted_items[:n] if score > 0]\n",
    "    \n",
    "    # Pad dengan None jika kurang dari 3\n",
    "    while len(top_emotions) < 3:\n",
    "        top_emotions.append(None)\n",
    "    \n",
    "    return top_emotions[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "822d2d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_emotions_list = df.apply(\n",
    "    lambda row: extract_top_emotions_separate(\n",
    "        row['empath_scores'], row['cleaned_statement'], row['label']\n",
    "    ), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5588ac47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data with separate emotion columns:\n",
      "                                           statement   label    top_1 top_2  \\\n",
      "0  Leaves are also standby in front of the PC ......  Normal  neutral  None   \n",
      "1  Bismillah for Eid 2021, you get a few question...  Normal  neutral  None   \n",
      "2  I want to spend a lot of time shopping for sna...  Normal  neutral  None   \n",
      "3  I like to be grateful, don't you think, if you...  Normal  neutral  None   \n",
      "4  why is this person blg parcel hampers blah bla...  Normal  neutral  None   \n",
      "5  I want to take a day off from work and then ta...  Normal  neutral  None   \n",
      "6  How many bbl tickets are now? How come I want ...  Normal  neutral  None   \n",
      "7  I bought a shirt for 200, but I've never worn ...  Normal  neutral  None   \n",
      "8  Chinese foreigners can enter at will because t...  Normal  neutral  None   \n",
      "9  09.35 WIB #Tol_JORR_E TMII - Cikunir - Cakung ...  Normal  neutral  None   \n",
      "\n",
      "  top_3  \n",
      "0  None  \n",
      "1  None  \n",
      "2  None  \n",
      "3  None  \n",
      "4  None  \n",
      "5  None  \n",
      "6  None  \n",
      "7  None  \n",
      "8  None  \n",
      "9  None  \n"
     ]
    }
   ],
   "source": [
    "df['top_1'] = [emotions[0] for emotions in top_emotions_list]\n",
    "df['top_2'] = [emotions[1] for emotions in top_emotions_list]\n",
    "df['top_3'] = [emotions[2] for emotions in top_emotions_list]\n",
    "\n",
    "print(\"Sample data with separate emotion columns:\")\n",
    "print(df[['statement', 'label', 'top_1', 'top_2', 'top_3']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea27883c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"top123.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "engmod",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
